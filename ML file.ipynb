{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b23b159-21ba-48ce-ae24-8879f1c6f102",
   "metadata": {},
   "source": [
    "## Now we move onto coding the ML model itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b89796e-8828-4692-a024-e23ac40fce5e",
   "metadata": {},
   "source": [
    "First we make a manifest of cleaned frames, this helps keep everything organised in one place and makes the whole process easier as we dont need to continuously scan our folders every time. The function below scans `frames_3fps_clean` folder and collects the image paths and agent names into a dataframe called `manifest_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32df6327-2016-4ab4-9df6-0e172a5f66e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using frames from: D:\\Valorant ML data\\frames_3fps_clean\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_DIR   = Path(r\"D:\\Valorant ML data\")\n",
    "FRAME_OUT_DIR = PROJECT_DIR / \"frames_3fps_clean\"\n",
    "\n",
    "print(\"Using frames from:\", FRAME_OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315150eb-2c2b-435e-bffa-9886a83d8724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images: 4671\n",
      "\n",
      "per-agent counts:\n",
      "agent\n",
      "Astra        185\n",
      "Breach       241\n",
      "Brimstone    305\n",
      "Chamber      347\n",
      "Cipher       337\n",
      "Clove        324\n",
      "Deadlock     299\n",
      "Fade         321\n",
      "Gekko        345\n",
      "Iso          356\n",
      "Jett         215\n",
      "Kayo         359\n",
      "Neon         335\n",
      "Raze         341\n",
      "Sage         361\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:\\Valorant ML data\\frames_3fps_clean\\Astra\\Va...</td>\n",
       "      <td>Astra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:\\Valorant ML data\\frames_3fps_clean\\Astra\\Va...</td>\n",
       "      <td>Astra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:\\Valorant ML data\\frames_3fps_clean\\Astra\\Va...</td>\n",
       "      <td>Astra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:\\Valorant ML data\\frames_3fps_clean\\Astra\\Va...</td>\n",
       "      <td>Astra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:\\Valorant ML data\\frames_3fps_clean\\Astra\\Va...</td>\n",
       "      <td>Astra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  agent\n",
       "0  D:\\Valorant ML data\\frames_3fps_clean\\Astra\\Va...  Astra\n",
       "1  D:\\Valorant ML data\\frames_3fps_clean\\Astra\\Va...  Astra\n",
       "2  D:\\Valorant ML data\\frames_3fps_clean\\Astra\\Va...  Astra\n",
       "3  D:\\Valorant ML data\\frames_3fps_clean\\Astra\\Va...  Astra\n",
       "4  D:\\Valorant ML data\\frames_3fps_clean\\Astra\\Va...  Astra"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_manifest(frames_root: Path):\n",
    "    rows = []\n",
    "    frames_root = Path(frames_root)\n",
    "    for agent_folder in sorted(frames_root.iterdir()):\n",
    "        if agent_folder.is_dir():\n",
    "            agent = agent_folder.name\n",
    "            for img_path in sorted(agent_folder.glob(\"*.jpg\")):\n",
    "                rows.append({\"filepath\": str(img_path), \"agent\": agent})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "manifest_df = build_manifest(FRAME_OUT_DIR)\n",
    "\n",
    "print(\"total images:\", len(manifest_df))\n",
    "print(\"\\nper-agent counts:\")\n",
    "print(manifest_df[\"agent\"].value_counts().sort_index().to_string())\n",
    "\n",
    "manifest_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d605e-945b-4fe3-a5eb-fc4ca8f6652e",
   "metadata": {},
   "source": [
    "Now we need to create a split for the data for the training, validation and test groups. I just arbitrarily chose a simple 70/15/15 split. I also use stratification so that each agent is included in all the groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97d01157-1214-439f-a5bf-8d08d25b1099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Split arrays or matrices into random train and test subsets.\n",
       "\n",
       "Quick utility that wraps input validation,\n",
       "``next(ShuffleSplit().split(X, y))``, and application to input data\n",
       "into a single call for splitting (and optionally subsampling) data into a\n",
       "one-liner.\n",
       "\n",
       "Read more in the :ref:`User Guide <cross_validation>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "*arrays : sequence of indexables with same length / shape[0]\n",
       "    Allowed inputs are lists, numpy arrays, scipy-sparse\n",
       "    matrices or pandas dataframes.\n",
       "\n",
       "test_size : float or int, default=None\n",
       "    If float, should be between 0.0 and 1.0 and represent the proportion\n",
       "    of the dataset to include in the test split. If int, represents the\n",
       "    absolute number of test samples. If None, the value is set to the\n",
       "    complement of the train size. If ``train_size`` is also None, it will\n",
       "    be set to 0.25.\n",
       "\n",
       "train_size : float or int, default=None\n",
       "    If float, should be between 0.0 and 1.0 and represent the\n",
       "    proportion of the dataset to include in the train split. If\n",
       "    int, represents the absolute number of train samples. If None,\n",
       "    the value is automatically set to the complement of the test size.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the shuffling applied to the data before applying the split.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "shuffle : bool, default=True\n",
       "    Whether or not to shuffle the data before splitting. If shuffle=False\n",
       "    then stratify must be None.\n",
       "\n",
       "stratify : array-like, default=None\n",
       "    If not None, data is split in a stratified fashion, using this as\n",
       "    the class labels.\n",
       "    Read more in the :ref:`User Guide <stratification>`.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "splitting : list, length=2 * len(arrays)\n",
       "    List containing train-test split of inputs.\n",
       "\n",
       "    .. versionadded:: 0.16\n",
       "        If the input is sparse, the output will be a\n",
       "        ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
       "        input type.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.model_selection import train_test_split\n",
       ">>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
       ">>> X\n",
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])\n",
       ">>> list(y)\n",
       "[0, 1, 2, 3, 4]\n",
       "\n",
       ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
       "...     X, y, test_size=0.33, random_state=42)\n",
       "...\n",
       ">>> X_train\n",
       "array([[4, 5],\n",
       "       [0, 1],\n",
       "       [6, 7]])\n",
       ">>> y_train\n",
       "[2, 0, 3]\n",
       ">>> X_test\n",
       "array([[2, 3],\n",
       "       [8, 9]])\n",
       ">>> y_test\n",
       "[1, 4]\n",
       "\n",
       ">>> train_test_split(y, shuffle=False)\n",
       "[[0, 1, 2], [3, 4]]\n",
       "\u001b[1;31mFile:\u001b[0m      d:\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f661ea6-cce0-4c0b-b949-195ba51b3559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split sizes:   train: 3269 | val: 701 | test: 701\n",
      "\n",
      "train counts:\n",
      "agent\n",
      "Astra        130\n",
      "Breach       169\n",
      "Brimstone    213\n",
      "Chamber      243\n",
      "Cipher       236\n",
      "Clove        227\n",
      "Deadlock     209\n",
      "Fade         225\n",
      "Gekko        241\n",
      "Iso          249\n",
      "Jett         150\n",
      "Kayo         251\n",
      "Neon         234\n",
      "Raze         239\n",
      "Sage         253\n"
     ]
    }
   ],
   "source": [
    "# 70% train and 30% temp\n",
    "train_df, temp_df = train_test_split(\n",
    "    manifest_df,\n",
    "    test_size=0.30,\n",
    "    stratify=manifest_df['agent'],\n",
    "    random_state=42)\n",
    "\n",
    "# split the remaining 30% into 15% val, 15% test (still stratified)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,    \n",
    "    stratify=temp_df['agent'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"split sizes:  \", \"train:\", len(train_df), \"| val:\", len(val_df), \"| test:\", len(test_df))\n",
    "\n",
    "# quick look at class balance in the train split\n",
    "print(\"\\ntrain counts:\")\n",
    "print(train_df['agent'].value_counts().sort_index().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e7b6e-22f7-4677-bc36-2c410780e255",
   "metadata": {},
   "source": [
    "There is quite a bit of variance here (I admit my data collection was not the best). Out of interest, I calculate summary stats of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4fbb49-b20e-4994-80c2-4a79e46956b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 217.93\n",
      "Variance: 1461.07\n",
      "Standard Deviation: 38.22\n"
     ]
    }
   ],
   "source": [
    "train_counts = train_df['agent'].value_counts().sort_index()\n",
    "\n",
    "mean_count = train_counts.mean()\n",
    "var_count  = train_counts.var()  \n",
    "\n",
    "print(\"Mean:\", round(mean_count, 2))\n",
    "print(\"Variance:\", round(var_count, 2))\n",
    "print(\"Standard Deviation:\", round(var_count**0.5, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bcb71c-c9de-4a14-82d2-dfd8e50a7c54",
   "metadata": {},
   "source": [
    "We now set up basic torchvision transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c62d9ec4-da1e-4ebf-95ab-98e9f47a4c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: ['Astra', 'Breach', 'Brimstone', 'Chamber', 'Cipher', 'Clove', 'Deadlock', 'Fade', 'Gekko', 'Iso', 'Jett', 'Kayo', 'Neon', 'Raze', 'Sage']\n",
      "num_classes: 15\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# plain list of classes in a fixed order\n",
    "class_names = sorted(train_df['agent'].unique())\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(\"classes:\", class_names)\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "# we then define variables for image transforms \n",
    "IMG_SIZE = 224\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfms = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),])\n",
    "\n",
    "eval_tfms = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63d10e-44aa-4720-b071-9194b2421463",
   "metadata": {},
   "source": [
    "We then create a small data set that takes image paths from the pandas data frame and :\n",
    "\n",
    "converts from BGR to RGB\n",
    "\n",
    "applies our transforms\n",
    "\n",
    "and returns a tuple `(image_tensor, label_int)`.\n",
    "\n",
    "Then we build train/val/test loaders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fc49826-7905-40cc-a418-3c1d9fafa855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes: train: 3269 | val: 701 | test: 701\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class FrameTableDataset(Dataset):\n",
    "    def __init__(self, table, class_names, transform=None):\n",
    "        self.table = table.reset_index(drop=True) #this makes the indexing clean\n",
    "        self.class_names = list(class_names)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.table)  #this returns no. of rows in the table\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.table.iloc[idx]\n",
    "        path = row['filepath']\n",
    "        label_name = row['agent']\n",
    "\n",
    "        # map label name to integer\n",
    "        label = self.class_names.index(label_name)\n",
    "\n",
    "        img_bgr = cv2.imread(path)\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(pil_img)\n",
    "        else:\n",
    "            x = torch.from_numpy(np.transpose(img_rgb, (2, 0, 1))).float() / 255.0\n",
    "        return x, int(label)\n",
    "\n",
    "# datasets\n",
    "train_ds = FrameTableDataset(train_df, class_names, transform=train_tfms)\n",
    "val_ds = FrameTableDataset(val_df, class_names, transform=eval_tfms)\n",
    "test_ds = FrameTableDataset(test_df, class_names, transform=eval_tfms)\n",
    "\n",
    "#dataloaders\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "test_loader = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "print(\"sizes:\", \"train:\", len(train_ds), \"| val:\", len(val_ds), \"| test:\", len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd8ec3a-93a8-4f09-8ddc-00abcc0f0258",
   "metadata": {},
   "source": [
    "Here I choose my device. Because I have an RTX 3060 Ti I obviously want to use cuda to speed up the training process. I also load the pretrained resnet18, however I need to adjust the final output layer to the number of agents (15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce0d8c39-0820-4fde-b9ec-d0323693b913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "in_feats = model.fc.in_features\n",
    "model.fc = nn.Linear(in_feats, num_classes) #this adjust the final layer\n",
    "\n",
    "# move to device, which is 3060 ti in my case\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d632668-52cb-4044-a7e9-3dbc44cc3f26",
   "metadata": {},
   "source": [
    "WE will run gradient descent with Adam. cross-entropy loss matches multi-class classification by maxxing the log likelihood of the correct class. We print average loss (how wrong on average) and accuracy each epoch. Online its recommended to use a small learning rate specifically when tuning a pre trained net.\n",
    "On the forward pass we `model(x)` calculateds raw class scores, `criterion(logits, y)` calculates the cross-entropy loss. Then on the backprop `loss.backward()` computes gradients, and `optimiser.step()` uses ADAM to update the weights.. \n",
    "\n",
    "After training I went back to also save important values, which will be used for grapnhing later (I wish I did this before I trained, so I wouldnt have to wait for model to re-train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04878a2-9716-4edc-a271-3cc557145e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3481e619-6b7e-4f50-94f2-b5e9a8a94478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train_one_epoch(model, loader, optimiser, device):\n",
    "    model.train()   #this puts the layers in training mode\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_seen = 0   #we set all this counters to 0 at first\n",
    "\n",
    "    for x, y in loader:  #here x=images and y=labels\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)  #offloads data to 3060 Ti for training\n",
    "\n",
    "        optimiser.zero_grad()  #clears old gradients to prevent them building up across batches\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)     #we add this batchs loss to the running total, we multiply by batch size\n",
    "        preds = logits.argmax(dim=1)       #the predicted class=index of the largest score per sample\n",
    "        total_correct += (preds == y).sum().item()    #this counts how many predictions match the true labels (y) in this batch\n",
    "        total_seen += x.size(0)     #this is just a counter of batches.\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_seen)    #need the max functon here or else could divide by 0\n",
    "    acc = total_correct / max(1, total_seen)\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f279471-fa39-49e0-a647-665ebb7ae275",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "#this fucntion evaluates on a loader(which is the val set here)\n",
    "def eval_one_loader(model, loader, device):\n",
    "    model.eval()   #eval mode\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x)     #forward pass only and no grads calculated\n",
    "        loss = criterion(logits, y)       #we need to keep track of loss here as we use val loss to spot overfitting\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == y).sum().item()\n",
    "        total_seen += x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_seen)\n",
    "    acc = total_correct / max(1, total_seen)\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "692e755d-8fbf-41b7-a45b-fc274f657708",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m val_acc_hist \u001b[38;5;241m=\u001b[39m []  \n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):   \u001b[38;5;66;03m#1-indexxed for nicer printing\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m train_one_epoch(model, train_loader, optimiser, DEVICE)\n\u001b[0;32m      9\u001b[0m     va_loss, va_acc \u001b[38;5;241m=\u001b[39m eval_one_loader(model, val_loader, DEVICE)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m --- val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimiser, device)\u001b[0m\n\u001b[0;32m      6\u001b[0m total_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m total_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m   \u001b[38;5;66;03m#we set all this counters to 0 at first\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m loader:  \u001b[38;5;66;03m#here x=images and y=labels\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m#offloads data to 3060 Ti for training\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m, in \u001b[0;36mFrameTableDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m pil_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img_rgb)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(pil_img)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mtranspose(img_rgb, (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresize(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mresize(\u001b[38;5;28mtuple\u001b[39m(size[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), interpolation)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\PIL\\Image.py:2328\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2316\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2317\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2318\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m   2319\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2320\u001b[0m         )\n\u001b[0;32m   2321\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2322\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2323\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2324\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2325\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2326\u001b[0m         )\n\u001b[1;32m-> 2328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim\u001b[38;5;241m.\u001b[39mresize(size, resample, box))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_list = [] \n",
    "train_loss_hist = [] \n",
    "val_loss_hist = []  \n",
    "train_acc_hist = [] \n",
    "val_acc_hist = []  \n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):   #1-indexxed for nicer printing\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimiser, DEVICE)\n",
    "    va_loss, va_acc = eval_one_loader(model, val_loader, DEVICE)\n",
    "    print(f\"epoch {epoch} - train loss {tr_loss:.4f}  acc {tr_acc:.4f} --- val loss {va_loss:.4f}  acc {va_acc:.4f}\")\n",
    "\n",
    "    epoch_list.append(epoch)          \n",
    "    train_loss_hist.append(tr_loss)  \n",
    "    val_loss_hist.append(va_loss)     \n",
    "    train_acc_hist.append(tr_acc)  \n",
    "    val_acc_hist.append(va_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f99f30-a50b-4608-aad5-24918f4c4553",
   "metadata": {},
   "source": [
    "Next we run the model on the test set with gradients off, then uses `argmax` as the predicted class. Under cross-entropy, argmax matches the class with the highest estimated probability. The per class table helps spot any issues with specific class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e58b1a-9d82-44f1-9834-3a6a2ab24fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model.eval()  #as we are doing inference\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        scores = model(x)         # raw class scores\n",
    "        preds = scores.argmax(1)          # pick class with highest score\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)  #we convert these to numpy arrays \n",
    "\n",
    "# overall accuracy\n",
    "test_acc = float((all_preds == all_labels).mean())  #we average over everyhting \n",
    "print(f\"test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# per agent accuracy \n",
    "counts = {name: {\"correct\": 0, \"total\": 0} for name in class_names}\n",
    "for true_lab, pred_lab in zip(all_labels, all_preds):   #iterates over tuples\n",
    "    name = class_names[int(true_lab)]\n",
    "    counts[name][\"total\"] += 1\n",
    "    if pred_lab == true_lab:\n",
    "        counts[name][\"correct\"] += 1\n",
    "\n",
    "rows = []\n",
    "for name in sorted(counts.keys()):\n",
    "    cor = counts[name][\"correct\"]\n",
    "    tot = counts[name][\"total\"]\n",
    "    acc = (cor / tot) if tot > 0 else float(\"nan\")  #to again prevent division by 0\n",
    "    rows.append({\"class\": name, \"correct\": cor, \"total\": tot, \"accuracy\": acc})\n",
    "\n",
    "per_class_df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\nper class accuracy:\")\n",
    "print(per_class_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c4efa-d7db-478f-912a-f7483122713d",
   "metadata": {},
   "source": [
    "Okay so the heavy lifting is done, we can now do things like save our model weights to the stated file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed5b53-d7b7-484c-9841-06080e5b7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = Path(r\"D:\\Valorant ML data\") / \"resnet18_valorant_updated.pth\"\n",
    "torch.save(model.state_dict(), WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1ff11-cbbc-4ff8-89ad-f649a04dfdc9",
   "metadata": {},
   "source": [
    "Using matplotlib, we plot 2 graphs, one for loss and one for accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147213c4-605a-4b0e-bbaf-a561ad3434fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch_list, train_acc_hist, label=\"train acc\")\n",
    "plt.plot(epoch_list, val_acc_hist,   label=\"val acc\")\n",
    "plt.xlabel(\"epoch\"); \n",
    "plt.ylabel(\"accuracy\"); \n",
    "plt.legend(); \n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch_list, train_loss_hist, label=\"train loss\")\n",
    "plt.plot(epoch_list, val_loss_hist,   label=\"val loss\")\n",
    "plt.xlabel(\"epoch\"); \n",
    "plt.ylabel(\"loss\"); \n",
    "plt.legend(); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54334375-ce65-4b76-a307-ead5778a8842",
   "metadata": {},
   "source": [
    "Lastly, I took some extra images of different characters on a different map (Bind) to try to test the models predictive capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3f9a5-8d25-4b88-8c93-f5865cc6f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict(image_path):\n",
    "    model.eval()  #again we are in inference mode\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    x = eval_tfms(img).unsqueeze(0).to(DEVICE)  #the shape is  1 x C x H x W\n",
    "    with torch.no_grad():\n",
    "        scores = model(x)    # computes class scores\n",
    "        pred_idx = int(scores.argmax(1).item()) # highest score will obviously be predicted class\n",
    "        prob = float(F.softmax(scores, dim=1)[0, pred_idx].item())\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"{class_names[pred_idx]}  (p={prob:.5f})\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238eade-928d-423b-bf3a-fb98754f2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(r\"D:\\Valorant ML data\\Screenshot (12).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ff111-6593-4b3b-805e-df707b0e7c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
